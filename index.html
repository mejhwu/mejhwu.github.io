<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.3.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.3.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="个人博客">
<meta property="og:url" content="https://mejhwu.github.io/index.html">
<meta property="og:site_name" content="个人博客">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="个人博客">



  <link rel="alternate" href="/atom.xml" title="个人博客" type="application/atom+xml" />




  <link rel="canonical" href="https://mejhwu.github.io/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>个人博客</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">个人博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />Home</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />Archives</a>
  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://mejhwu.github.io/2018/06/27/gradient_descent/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="jhwu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/27/gradient_descent/" itemprop="url">
                  Untitled
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-06-27 20:18:12" itemprop="dateCreated datePublished" datetime="2018-06-27T20:18:12+08:00">2018-06-27</time>
            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <hr>
<p>title: 梯度下降<br>date:  2018/4/24</p>
<h2 id="categories-机器学习"><a href="#categories-机器学习" class="headerlink" title="categories: 机器学习"></a>categories: 机器学习</h2><p>梯度下降就是在函数当前点梯度(偏导)的反方向的规定步长进行迭代,直至收敛(即到达局部最小值).</p>
<p>批量梯度下降</p>
<p>假设有损失函数</p>
<p>$$J(\theta)=\frac{1}{2}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$$</p>
<p>其中$h_\theta(x)=\theta_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_nx_n$.</p>
<p>那么</p>
<p>$$\theta_j:=\theta_j - \alpha \frac{\partial}{\partial \theta_j}J(\theta)$$</p>
<p>$$\begin{aligned} \frac{\partial}{\partial \theta_j}J(\theta) = \sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \end{aligned}$$</p>
<p>所以</p>
<p>$$\theta_j:=\theta_j + \sum_{i=1}^{m}( y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)}$$</p>
<p>以上公示就是批量梯度下降,每一次迭代$\theta_j$时,都需要完全遍历整个输入集合.</p>
<p>随机梯度下降</p>
<p>假设只有一个输入样本,则</p>
<p>$$\theta_j:=\theta_j + ( y - h_\theta(x)x_j$$</p>
<p>以上就是随机梯度下降的公式.即每次更新$\theta_j$只用一个样本.</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://mejhwu.github.io/2018/06/27/decision_tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="jhwu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/27/decision_tree/" itemprop="url">
                  决策树
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-06-27 00:00:00 / Modified: 20:18:02" itemprop="dateCreated datePublished" datetime="2018-06-27T00:00:00+08:00">2018-06-27</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><h2 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1. 基本概念"></a>1. 基本概念</h2><p>决策树是一种常见的机器学习方法,一般一棵决策树为一颗多叉树.<br>每一个叶子节点就对应于一个决策结果.决策树的生成过程类似于数据结构中的树的生成过程.</p>
<hr>
<p>输入:</p>
<p>训练集$D={(x_1,y_1),(x_2,y_2),…,(x_m,y_m)}$</p>
<p>属性集$A={a_1,a_2,…,a_d}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">西瓜书基本算法:</span><br><span class="line">过程: 函数TreeGenerate(D, A)</span><br><span class="line">生成节点node</span><br><span class="line"><span class="keyword">if</span> D中样本全属于同一类别C then</span><br><span class="line">    将node标记为C类叶节点; <span class="keyword">return</span></span><br><span class="line">end <span class="keyword">if</span></span><br><span class="line"><span class="keyword">if</span> A为空 OR D中样本在A上的取值相同 then</span><br><span class="line">    将node标记为叶节点,其分类标记为D中样本数最多的类; <span class="keyword">return</span></span><br><span class="line">end <span class="keyword">if</span></span><br><span class="line">从A中选择最优的划分属性a;</span><br><span class="line"><span class="keyword">for</span> a 的每一个值av do</span><br><span class="line">    为node生成一个分支;令Dv表示D中在a上取值为av的样本集;</span><br><span class="line">    <span class="keyword">if</span> Dv 为空 then</span><br><span class="line">        将分支标记为叶节点,其类别标记为D中样本最多的类; <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        以TreeGenerate(Dv, A-&#123;a*&#125;)为分支节点</span><br><span class="line">    end <span class="keyword">if</span></span><br><span class="line">end <span class="keyword">for</span></span><br></pre></td></tr></table></figure>
<p>输出: 以node为根节点的一颗决策树</p>
<hr>
<p>以下为python代码的伪代码, 参考&lt;&lt;机器学习实战&gt;&gt;</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_tree</span><span class="params">(data_set, labels)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> D中样本全输入同一类别C:</span><br><span class="line">        <span class="keyword">return</span> 类别C</span><br><span class="line">    <span class="keyword">if</span> A为空:</span><br><span class="line">        <span class="keyword">return</span> D中样本数最多的类</span><br><span class="line">    从A中选取最优划分属性a</span><br><span class="line">    node = &#123;label: &#123;&#125;&#125;</span><br><span class="line">    <span class="keyword">for</span> a的每一个属性值av:</span><br><span class="line">        node[label][av] = &#123;&#125;</span><br><span class="line">        令Dv表示D在属性a上取值av的样本子集</span><br><span class="line">        <span class="keyword">if</span> Dv为空:</span><br><span class="line">            node[label][av] = D中样本最多的类</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            node[label][av] = create_tree(Dv, labels)</span><br><span class="line">    <span class="keyword">return</span> node</span><br></pre></td></tr></table></figure>
<p>在&lt;&lt;西瓜书&gt;&gt;中有三种情况导致递归返回:(1)当前节点包含的样本全属于同一类别, 无需划分;(2)当前属性集为空,或是所有样本在所有属性上取值相同,无法划分;(3)当前节点包含的样本集合为空,不能划分</p>
<p>在&lt;&lt;机器学习实战&gt;&gt;没有判断”所有样本在所有属性上取值相同”这个条件,个人认为原因有两个: 其一, 判断的难度比较大,代码复杂,耗费时间长; 其二, 在满足条件”所有样本在所有属性上取值相同”这个条件时, 其所有样本类别有很大概率是属于同一类, 再者继续训练也只会形成一个单叉树.</p>
<h2 id="2-划分选择"><a href="#2-划分选择" class="headerlink" title="2. 划分选择"></a>2. 划分选择</h2><p>在以上的算法流程中,最重要的步骤就是在属性集A中选择最优的划分属性a, 一般在划分的过程中,希望划分出来的样本子集尽量属于同一类别, 即节点的”纯度”越来越高.</p>
<h3 id="2-1-信息增益"><a href="#2-1-信息增益" class="headerlink" title="2.1 信息增益"></a>2.1 信息增益</h3><p><a href="https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA" target="_blank" rel="noopener">“信息熵”</a>)是度量样本集合纯度最常用的一种指标. 假定当前样本集合D中第$k$类样本的比例为$p_k(k=1,2,…,|\mathcal{Y}|)$, 则$D$的信息熵定义为</p>
<p>$$Ent(D)= \sum ^{|\mathcal{Y}|}_{k=1}p_klog_2p_k$$</p>
<p>$Ent(D)$的值越小,则$D$的纯度越高.</p>
<p>计算信息熵时约定:若$p=0$, 则$plog_2p=0$. $Ent(D)$的最小值为0,最大值为$log_2|\mathcal{Y}|$</p>
<p>下面给出信息增益的计算公式</p>
<p>$$Gain(D, a)=Ent(D)- \sum ^V_{v=1} \frac {|D|}{|D^v|}Ent(D^v)$$</p>
<p>$V{a^1,a^2,…,a^v}$为属性$a$的属性值集合; $D^v$为使用属性$a$在$D$中进行划分,$D$中属性$a$的属性值为$a^v$的样本子集;  $\frac{|D|}{|D^v|}$为每个分支节点上的权重.</p>
<p>一般而言, 信息增益越大, 则意味着使用属性$a$来进行划分所获得的”纯度提升”越大. 所有在算法中选择属性$a_*=arg_{a \in A} maxGain(D, a)$</p>
<p>python代码<a href="https://github.com/mejhwu/machine_learning/blob/master/decision_tree/tree_gain.py" target="_blank" rel="noopener">tree_gain.py</a></p>
<h3 id="2-2-增益率"><a href="#2-2-增益率" class="headerlink" title="2.2 增益率"></a>2.2 增益率</h3><p>信息增益准则对可取值较多的属性有所偏好,为减少这种偏好可能带来的不利影响,可使用”增益率”来选择最优划分属性, 增益率定义为</p>
<p>$$Gain_ratio(D,a)=\frac{Grain(D,a)}{IV(a)}$$</p>
<p>其中</p>
<p>$$IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}$$</p>
<p>$IV(a)$称为属性$a$的”固有值”(intrinsic value). 属性$a$的可能取值数目越多(即V越大), 则$IV(a)$的值通常会越大.</p>
<p>需要注意的是, 增益率准则对可取值数目较少的属性所有偏好, 因此根据C4.5算法, 可先从候选划分属性中找出信息增益高于平均水平的属性,再从中选择增益率最高的.</p>
<p>python代码<a href="https://github.com/mejhwu/machine_learning/blob/master/decision_tree/tree_gain_ratio.py" target="_blank" rel="noopener">tree_gain_ratio.py</a></p>
<h3 id="2-3-基尼指数"><a href="#2-3-基尼指数" class="headerlink" title="2.3 基尼指数"></a>2.3 基尼指数</h3><p>CART决策树使用”基尼指数”(Gini index)来选择划分属性. 数据集$D$的纯度可以用基尼值来度量:</p>
<p>$$Gini(D)=\sum_{k=1}^{|\mathcal{Y}|}\sum_{k{‘}\ne k}p_kp_{k^{‘}}=1-\sum_{k=1}^{|\mathcal{Y}|}p_k^2$$</p>
<p>直观来说,$Gini(D)$反映了从数据集$D$中随机抽取两个样本,其类别不一致的概率. 因此, $Gini(D)$越小, 则数据集$D$的纯度越高.</p>
<p>属性$a$的基尼指数定义为</p>
<p>$$Gini_index(D,a)=\sum_{k=1}^{|\mathcal{Y}|}\frac{|D^v|}{|D|}Gini(D^v)$$</p>
<p>于是,在候选属性集$A$中选取使划分后基尼指数最小的属性作为最优划分属性,即$a_*=arg_{a\in A}min\ Gini_index(D,a)$</p>
<p>python代码<a href="https://github.com/mejhwu/machine_learning/blob/master/decision_tree/tree_gini.py" target="_blank" rel="noopener">tree_gini.py</a></p>
<h2 id="3-剪枝处理"><a href="#3-剪枝处理" class="headerlink" title="3. 剪枝处理"></a>3. 剪枝处理</h2><p>剪枝(pruning)是决策树学习算法中对付”过拟合”的主要手段. 决策树剪枝的基本策略有”预剪枝”(prepruning)和”后剪枝”(postpruning). </p>
<h3 id="3-1-预剪枝"><a href="#3-1-预剪枝" class="headerlink" title="3.1 预剪枝"></a>3.1 预剪枝</h3><p>预剪枝是指在决策树生成过程中,对每个节点在划分前先进行估计, 若当前节点的划分不能带来决策树泛化性能提升, 则停止划分并讲当前节点标记为页节点.</p>
<p>预剪枝的决策树生成过程类似于二叉树的先序遍历, 划分前先进行判断是否剪枝, 如果不需要剪枝再生成下一个节点.</p>
<p>预剪枝基于”贪心”本质禁止这些分支展开,给预剪枝决策树带来了欠拟合的风险.</p>
<p>预剪枝python伪代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">verity_divide</span><span class="params">(train_data_set, train_data_set)</span>:</span></span><br><span class="line">    <span class="comment"># 验证集为空不进行划分</span></span><br><span class="line">    <span class="keyword">if</span> 验证集为空:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">    选取最优划分属性a</span><br><span class="line">    划分后节点divide_node = &#123;a: &#123;&#125;&#125;</span><br><span class="line">    <span class="keyword">for</span> a的每一个属性值av:</span><br><span class="line">        令TDv表示训练样本train_data_set中属性a上取值为av的样本子集</span><br><span class="line">        divide_node[a][av] = TDv中类别最多的类</span><br><span class="line">    divide_count表示划分后验证的正确数量</span><br><span class="line">    <span class="keyword">for</span> train_data_set中的每一个样本:</span><br><span class="line">        <span class="keyword">if</span> 验证正确:</span><br><span class="line">            divide_cout += <span class="number">1</span></span><br><span class="line">    not_divide_count表示train_data_set中样本最多的类的数量</span><br><span class="line">    <span class="keyword">if</span> divide_count &gt; not_divide_count:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_tree</span><span class="params">(train_data_set, verity_data_set, labels)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> train_data_set中样本全输入同一类别C:</span><br><span class="line">        <span class="keyword">return</span> 类别C</span><br><span class="line">    <span class="keyword">if</span> A为空:</span><br><span class="line">        <span class="keyword">return</span> train_data_set中样本数最多的类</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> verity_divide(train_data_set, verity_data_set):</span><br><span class="line">        <span class="keyword">return</span> D中样本最多的类</span><br><span class="line">    <span class="comment"># 此处可优化, 可先获取最优属性后传入verity_divide()</span></span><br><span class="line">    从A中选取最优划分属性a</span><br><span class="line">    node = &#123;label: &#123;&#125;&#125;</span><br><span class="line">    <span class="keyword">for</span> a的每一个属性值av:</span><br><span class="line">        node[label][av] = &#123;&#125;</span><br><span class="line">        令TDv表示train_data_set在属性a上取值av的样本子集</span><br><span class="line">        令TVv表示verity_data_set在属性a上取值为av的样本子集</span><br><span class="line">        <span class="keyword">if</span> TDv为空:</span><br><span class="line">            node[label][av] = train_data_set中样本最多的类</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            node[label][av] = create_tree(TDv, TVv,  labels)</span><br><span class="line">    <span class="keyword">return</span> node</span><br></pre></td></tr></table></figure>
<p>完整代码<a href="https://github.com/mejhwu/machine_learning/blob/master/decision_tree/tree_gain_prepruning.py" target="_blank" rel="noopener">tree_gain_prepruning.py</a></p>
<h3 id="3-2-后剪枝"><a href="#3-2-后剪枝" class="headerlink" title="3.2 后剪枝"></a>3.2 后剪枝</h3><p>后剪枝是先从训练集生成一颗完整的决策树, 然后自底向上地对非叶子节点进行考察, 若将该节点对应的子树替换为叶节点能带来决策树的泛化性能提升,则将该子树替换为叶节点.</p>
<p>后剪枝决策树的生成过程类似于二叉树的后续遍历; 即先生成决策树, 在判断是否需要剪枝, 如果需要剪枝则放弃子树, 直接将节点标记为叶节点.</p>
<p>后剪枝的过程是在完全决策树之后进行的,并且要自底向上地对决策树中的所有非叶节点进行逐一考察, 因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多.</p>
<p>后剪枝python伪代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">verity_divide</span><span class="params">(tree, train_data_set, verity_data_set)</span>:</span></span><br><span class="line">    <span class="comment"># 验证集为空不剪枝</span></span><br><span class="line">    <span class="keyword">if</span> 验证集为空:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">    not_divide_right_rate = 不划分的验证正确率</span><br><span class="line">    divide_right_rate = 划分后的验证正确率</span><br><span class="line">    <span class="comment"># 不划分的验证正确率大于划分的验证正确率时剪枝</span></span><br><span class="line">    <span class="keyword">if</span> not_divide_right_rate &gt; divide_right_rate:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_tree</span><span class="params">(train_data_set, verity_data_set, labels)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> train_data_set中样本全输入同一类别C:</span><br><span class="line">        <span class="keyword">return</span> 类别C</span><br><span class="line">    <span class="keyword">if</span> A为空:</span><br><span class="line">        <span class="keyword">return</span> train_data_set中样本数最多的类</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> verity_divide(train_data_set, verity_data_set):</span><br><span class="line">        <span class="keyword">return</span> train_data_set中样本最多的类</span><br><span class="line">    从A中选取最优划分属性a</span><br><span class="line">    node = &#123;label: &#123;&#125;&#125;</span><br><span class="line">    <span class="keyword">for</span> a的每一个属性值av:</span><br><span class="line">        node[label][av] = &#123;&#125;</span><br><span class="line">        令TDv表示train_data_set在属性a上取值av的样本子集</span><br><span class="line">        令TVv表示verity_data_set在属性a上取值av的样本子集</span><br><span class="line">        <span class="keyword">if</span> TDv为空:</span><br><span class="line">            node[label][av] = train_data_set中样本最多的类</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            node[label][av] = create_tree(TDv, TVv, labels)</span><br><span class="line">    <span class="keyword">if</span> verity_divide(node, train_data_set, verity_data_set):</span><br><span class="line">        node = train_data_set中样本最多的类</span><br><span class="line">    <span class="keyword">return</span> node</span><br></pre></td></tr></table></figure>
<p>完整代码<a href="https://github.com/mejhwu/machine_learning/blob/master/decision_tree/tree_gain_postpruning.py" target="_blank" rel="noopener">tree_gain_postpruning</a></p>
<h2 id="4-连续与缺失值"><a href="#4-连续与缺失值" class="headerlink" title="4. 连续与缺失值"></a>4. 连续与缺失值</h2><h3 id="4-1-连续值处理"><a href="#4-1-连续值处理" class="headerlink" title="4.1 连续值处理"></a>4.1 连续值处理</h3><p>由于连续属性的可取值数目不再有限, 因此,不能直接根据连续属性的可取值来对节点进行划分, 需要将连续属性离散化, 最简单的策略是采用二分法对连续属性进行处理.</p>
<p>给定样本集$D$和连续属性$a$, 假定$a$在$D$上出现了n个不同的取值, 将这些值从小到大进行排序, 记为${a^1,a^1,…,a^n}$. 基于划分点$t$可将D分为子集$D_t^-$和$D_t^+$, 其中$D_t^-$包含那行属性a上取值不大于t的样本, 而$D_t^+$则包含那些在属性$a$上取值大于$t$的样本. 显然, 对相邻的属性取值$a^i$和$a^{i+1}$来说, $t$在区间$[a^i,a^{i+1})$中取任何值产生的划分结果相同. 因此, 对连续属性$a$, 我们可考察包含$n-1$个元素的候选划分点集合</p>
<p>$$T_a={\frac{a^i+a^{i+1}}{2}|i\le i\le n-1|}$$</p>
<p>即把区间$[a^i,a^{i+1})$的中位点$\frac{a^i+a^{i+1}}{2}$作为候选划分点. 然后就可像离散属性一样来考察这些划分点, 选取最优的划分点进行样本集合的划分.</p>
<p>$$Gain(D,a)=\max\limits_{t\in T_a} Ent(D) - \sum_{\lambda\in {-,+}} \frac{|D|}{|D_t^\lambda|}Ent(D_t^\lambda)$$</p>
<p>其中$Gain(D,a,t)$是样本集$D$基于划分点$t$二分后的信息增益. 于是,可选择$Gain(D,a,t)$最大化的划分点.</p>
<p>需要注意, 与离散值不同, 若当前节点划分属性为连续属性, 连续属性还可作为其后代节点的划分属性.</p>
<p>在写代码的时候需要注意在离散属性和连续属性的$gain(D,a)$时需要分开处理. 在构建决策树时, 离散属性和连续属性也需要分开处理, 因为划分连续属性时,不需要在数据集中去除连续属性.</p>
<p>完整python代码<a href="https://github.com/mejhwu/machine_learning/blob/master/decision_tree/tree_gain_continuous_value.py" target="_blank" rel="noopener">tree_gain_continuous_value.py</a></p>
<h3 id="4-2-缺失值处理"><a href="#4-2-缺失值处理" class="headerlink" title="4.2 缺失值处理"></a>4.2 缺失值处理</h3><p>面对缺失值需要解决的两个问题: (1)如何在属性值缺失的情况下进行划分属性选择? (2)给定划分属性,若样本在改属性上的值缺失,如何对样本进行划分?</p>
<p>给定训练集$D$和属性$a$, 令$\tilde{D}$表示$D$中在属性$a$上没有缺失值的样本子集. 对问题(1), 显然仅可根据$\tilde{D}$来判断属性$a$的优劣. 假定属性$a$有$V$个可取值${a^1,a^2,…,a^V}$, 令$\tilde{D}^v$表示$\tilde{D}$在属性$a$上的取值为$a^v$的样本子集, $\tilde{D}<em>k$表示$\tilde{D}$属性第$k$类$(k=1,2,…,|\mathcal{Y}|)$的样本子集, 则显然有$\tilde{D}=\cup</em>{k=1}^{|\mathcal{Y}|}\tilde{D}<em>k$, $\tilde{D}=\cup</em>{v=1}^V\tilde{D}^v$. 假定我们为每个样本$\boldsymbol{x}$赋予一个权重$w_{\boldsymbol{x}}$, 并定义</p>
<p>$$\rho=\frac{\sum_{\boldsymbol{x}\in \tilde{D}}w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x}\in D}w_{\boldsymbol{x}}}$$</p>
<p>$$\tilde{p}<em>k=\frac{\sum</em>{\boldsymbol{x}\in \tilde{D}<em>k}w</em>{\boldsymbol{x}}}{\sum_{\boldsymbol{x}\in \tilde{D}}w_{\boldsymbol{x}}} \ \ \ \ \ (1\le k\le |\mathcal{Y}|)$$</p>
<p>$$\tilde{r}<em>v=\frac{\sum</em>{\boldsymbol{x}\in \tilde{D}^v}w_{\boldsymbol{x}}}{\sum_{\boldsymbol{x}\in \tilde{D}}w_{\boldsymbol{x}}} \ \ \ \ \ (1\le v\le V)$$</p>
<p>对属性$a$, $\rho$表示无缺失值样本所占的比例, $\tilde{p}_k$表示无缺失值样本中第$k$类所占的比例, $\tilde{r}<em>v$则表示无缺失值样本中属性a上取值$a^v$的样所占的比例. 显然$\sum</em>{k=1}^{|\mathcal{Y}|}\tilde{p}<em>k=1$, $\sum</em>{v=1}^V\tilde{r}_v=1$</p>
<p>基于上述定义, 可将信息增益的计算式推广为</p>
<p>$$Gian(D,a)=\rho \times Gain(\tilde{D},a)=\rho \times (Ent(\tilde{D}) - \sum_{v=1}^V \tilde{r}_v Ent(\tilde{D}^v))$$</p>
<p>其中</p>
<p>$$Ent(\tilde{D}) = - \sum_{k=1}^{|\mathcal{Y}|}\tilde{p}_k log_2 \tilde{p}_k$$</p>
<p>对问题(2), 若样本$\boldsymbol{x}$在划分属性$a$上的取值已知, 则将$\boldsymbol{x}$划入与其取值对应的子节点, 且样本权值在子节点中保持$w_{\boldsymbol{x}}$. 若样本$\boldsymbol{x}$在划分属性$a$上的取值未知, 则将$\boldsymbol{x}$同时划入所有子节点, 且样本权值在与属性$a^v$对应的子节点中调整为$\tilde{r}<em>v \cdot w</em>{\boldsymbol{x}}$; 直观地看, 就是让同一样本以不同的概率划入到不同的子节点中去.</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://mejhwu.github.io/2018/04/22/conception_of_mathematical_statistics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="jhwu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/22/conception_of_mathematical_statistics/" itemprop="url">
                  数理统计的基本概念
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-04-22 00:00:00" itemprop="dateCreated datePublished" datetime="2018-04-22T00:00:00+08:00">2018-04-22</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-06-27 20:17:51" itemprop="dateModified" datetime="2018-06-27T20:17:51+08:00">2018-06-27</time>
              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/数学/" itemprop="url" rel="index"><span itemprop="name">数学</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="总体与样本"><a href="#总体与样本" class="headerlink" title="总体与样本"></a>总体与样本</h1><h2 id="总体-个体-样本"><a href="#总体-个体-样本" class="headerlink" title="总体    个体    样本"></a>总体    个体    样本</h2><p>一般将研究对象的全体组成的集合成为<strong>总体</strong>,组成总体的每一个成员成为<strong>个体</strong>.</p>
<p>从总体中挑选一部分个体的过程叫<strong>样本</strong>, 样本所含的个体数称为<strong>样本容量</strong>.</p>
<h2 id="抽样方法"><a href="#抽样方法" class="headerlink" title="抽样方法"></a>抽样方法</h2><p>抽样要保证对每一个个体”机会均等”,即总体中每一个体有同样机会被抽到,谁也不占优.凡是满足这个要求的抽样叫做<strong>随机抽样</strong>.</p>
<p>常用随机抽样方案:</p>
<p>(1)”集团抽样”.即先把总体中的全部个体,按某种考虑分成一些大集团,每个大集团内又可分为若干小集团,后者还可以再细分.抽样时,先用随机的方法抽取若干个大集团,再在抽出的每个大集团内分别抽出若干个小集团,$\cdots \cdots$这样下去,最后在最低一级的集团中随机抽出若干个体.这样抽出的全部个体构成所需的样本.</p>
<p>(2)”分层按比例抽样”.必须有两个条件:一是分层的标准应合理.这主要是指层与层之间确实有较大差异,而每层内各个个体的差异较小.二是每层所含个体数在总体全部个体数所占的比例要能够比较确切地知道.</p>
<h1 id="样本分布和统计量"><a href="#样本分布和统计量" class="headerlink" title="样本分布和统计量"></a>样本分布和统计量</h1><h2 id="样本分布"><a href="#样本分布" class="headerlink" title="样本分布"></a>样本分布</h2><p>样本是按照一定的方法从总体中抽出的一部分个体所组成的集合.样本总所含的个体数称为<strong>样本容量</strong>或<strong>样本大小</strong>.</p>
<p>将$n$个抽象的随机变量$X_1,X_2,\cdots,X_n$称为样本,而把具体的观测数字$(x_1,x_2,\cdots,x_n)$看成该样本的一组取值.</p>
<p>样本$(X_1,X_2,\cdots,X_n)$既然是随机变量,也就有其概率分布.样本的概率分布称为<strong>样本分布</strong>.</p>
<p>作为随机变量的$X_1,X_2,\cdots,X_n$可以认为是相互独立且有相同的分布,每一个$X_i$的分布都与总体$X$有相同的分布.</p>
<p>“随机抽样”的要求保证了样本分量$X_1$与总体$X$同分布的性质,”每次抽取时,总体成分保持不变”的要求保证了样本的各分量$X_1,\cdots,X_n$相互独立且都与总体$X$有相同分布的性质.</p>
<p>统计学中将”每次抽取时,总体的成分保持不变的随机抽样”称为<strong>简单随机抽样</strong>.</p>
<p>由简单随机抽样得到的样本称为<strong>随机样本</strong>.</p>
<h2 id="统计量"><a href="#统计量" class="headerlink" title="统计量"></a>统计量</h2><p>对样本进行必要的加工和运算处理后得到的结果称为<strong>统计量</strong>.</p>
<p>常用统计量:</p>
<ol>
<li>样本均值</li>
</ol>
<p>$$\bar{X}=\frac{1}{n}\sum_{i=1}{n}X_k$$</p>
<ol start="2">
<li>样本方差</li>
</ol>
<p>$$S^2=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X})^2$$</p>
<ol start="3">
<li>样本标准差</li>
</ol>
<p>$$S=\sqrt{S}=\sqrt{\frac{1}{n-1}\sum_{i=1}{n}(X_i-\bar{X})^2}$$</p>
<ol start="4">
<li>样本$k$阶原点矩</li>
</ol>
<p>$$A_k=\frac{1}{n}\sum_{i=1}^nX_i^k$$</p>
<ol start="5">
<li>样本$k$阶中心矩</li>
</ol>
<p>$$B_k=\frac{1}{n}\sum_{i=1}{n}(X_i-\bar{X})^k$$</p>
<ol start="6">
<li>从总体$X$中抽取一容量为$n$的样本$X_1,X_2,\cdots,X_n$,设相应的观测值为$x_1,\cdots,x_n$,将观测值按由小到大的次序重新排列为</li>
</ol>
<p>$$x_{(1)} \le x_{(2)} \le \cdots \le x_{(n)}$$</p>
<p>则称$x_{(1)},\cdots,x_{(n)}$为原始样本观测值$x_1,\cdots,x_n$的<strong>次序样本观测值</strong>.$x_{(1)},\cdots,x_{(n)}$由$x_1,\cdots,x_n$确定,$x_1,\cdots,x_n$的值有一定随机性,导致$x_{(1)},\cdots,x_{(n)}$的值也有一定随机性.为此,将次序样本观测值$x_{(1)},\cdots,x_{(n)}$看成(想象成)某$n$个随机变量$X_{(1)},\cdots,X_{(n)}$(其分布可能与$X_1,\cdots,X_n)$截然不同)的观测值,如此定义的$n$为随机变量$(X_{(1)},\cdots,X_{(n)})$被称为原始样本$X_1,\cdots,X_n)$的<strong>次序统计量</strong>.</p>
<ol start="7">
<li>样本中位数</li>
</ol>
<p>$$\tilde{X}=\begin{cases} X_(k+1) &amp; \text{if} \quad n=2k+1 \ \frac{1}{2}(X_{(k)} + X_{(k+1)}) &amp; \text{if} \quad n=2k \end{cases}$$</p>
<ol start="8">
<li>样本极差</li>
</ol>
<p>$$R=X_{(n)}-X_{(1)}$$</p>
<p>统计量的实质在于:统计量只依赖与样本$X_1,\cdots,X_n$,而不涉及任何其他未知的量,即它是样本的已知函数$g(X_1,\cdots,X_n)$,且不能含有任何未知参数.</p>
<h2 id="统计量的分布"><a href="#统计量的分布" class="headerlink" title="统计量的分布"></a>统计量的分布</h2><ol>
<li>$\chi^2$分布</li>
</ol>
<p>设$X_1,\cdots,X_n$相互独立且都服从$N(0,1)$分布,它们的平方和</p>
<p>$$\chi^2 \stackrel{def} = sX_1^2+ \cdots + X_n^2$$</p>
<p>的分布称为自由度为$n$的$\chi^2$分布,记为$\chi^2 \sim \chi(n)$.</p>
<p>其概率密度</p>
<p>$$f(y)=\begin{cases} \frac{1}{2^{\frac{n}{2}}\Gamma(\frac{n}{2})}y^{\frac{n}{2}-1} e^{-\frac{y}{2}}, &amp; y \ge 0 \ 0, &amp; y &lt; 0 \end{cases}$$</p>
<p>其中$\Gamma(z)=\int_0^{\infty}u^{z-1}e^{-u}du,(z\ge 0)$.</p>
<p>对任意给定的正数$\alpha(0&lt;\alpha&lt;0)$,称满足条件</p>
<p>$$\int_{\chi_\alpha^2(n)}^{\infty}f(y)dy=\alpha$$</p>
<p>的点$\chi_\alpha^2(n)$为$\chi^2(n)$的上$\alpha$分位点.</p>
<p>$\chi_\alpha^2(n)$的概率意义是:服从$\chi_\alpha^2(n)$分布的随机变量$\chi^2$,取值大于$\chi_\alpha^2(n)$的概率正好等于$\alpha$即</p>
<p>$$P{\chi^2 &gt; \chi_\alpha^2(n)}=\alpha$$</p>
<p>若$\chi_1^2 \sim \chi^2(n_1), \chi_2^2 \sim \chi^2(n_2)$,且$\chi_1^2$与$\chi_2^2$独立,则</p>
<p>$$\chi_1^2+\chi_2^2=\chi^2(n_1+n_2)$$</p>
<ol start="2">
<li>$t$分布</li>
</ol>
<p>设$X\sim N(0,1), Y\sim \chi^2(n)$,且$X,Y$相互独立,则随机变量</p>
<p>$$t\stackrel{def} =  \frac{X}{\sqrt{Y/n}}$$</p>
<p>的分布称为自由度$n$的$t$分布,记为$t \sim t(n)$.</p>
<p>$t(n)$分布的概率密度为</p>
<p>$$f(x)=\frac{\Gamma(\frac{n+1}{2})}{\sqrt{n\pi}\Gamma(\frac{n}{2})}(1+\frac{t^2}{n})^{-\frac{n+1}{2}}, \qquad -\infty &lt; t &lt; +\infty$$</p>
<p>其中$\Gamma(z)=\int_0^{\infty}u^{z-1}e^{-u}du,(z\ge 0)$.</p>
<ol start="3">
<li>$F$分布</li>
</ol>
<p>设$U\sim \chi^2(n_1), V\sim \chi^2(n_2)$,且$U,V$相互独立,则随机变量</p>
<p>$$F \stackrel{def} =  \frac{U/n_1}{V/n_2}$$</p>
<p>的分布称为自由度$(n_1,n_2)$的$F$分布,记为$F\sim F(n_1,n_2)$. 其中$n_1,n_2$分别称为第一,第二自由度.</p>
<p>$F(n_1,n_2)$分布的概率密度为</p>
<p>$$f(y)=\begin{cases} \frac{\Gamma(\frac{n_1+n_2}{2})}{\Gamma(\frac{n_1)}{2}\Gamma(\frac{n_2)}{2}} (\frac{n_1}{n_2})(\frac{n_1}{n_2}y)^{\frac{n_1}{2}-1}(1+\frac{n_1}{n_2}y)^{-\frac{n_1+n_2}{2}}, &amp; y \ge 0 \ 0 &amp; y &lt; 0 \end{cases}$$</p>
<p>$F(n_1,n_2)$分布的上$\alpha$分为点定义为满足条件</p>
<p>$$\int_{F_\alpha(n_1,n_2)}^{\infty}f(y)dy=\alpha$$</p>
<p>的点$F_\alpha(n_1,n_2)$. 并且</p>
<p>$$F_{1-\alpha}(n_2,n_2)=\frac{1}{F_\alpha(n_1,n_2)}$$</p>
<h2 id="与正态总体统计量有关的分布"><a href="#与正态总体统计量有关的分布" class="headerlink" title="与正态总体统计量有关的分布"></a>与正态总体统计量有关的分布</h2><ol>
<li>若$X$服从一元正态分布$N(\mu, \sigma^2)$,则$X$的线性函数</li>
</ol>
<p>$$aX+b \sim N(a\mu+b, a^2 \sigma^2)$$</p>
<p>特别</p>
<p>$$\frac{X-\mu}{\sigma} \sim N(0,1)$$</p>
<ol start="2">
<li>若$X \sim N(\mu_1, \sigma_1^2), Y \sim N(\mu_2, \sigma_2^2)$,且$X$与$Y$相互独立,则</li>
</ol>
<p>$$X \pm Y \sim N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$$</p>
<ol start="3">
<li>如果随机变量$\mathbf{X}$服从均值向量为$\mathbf{\mu}$,协方差矩阵为$\mathbf{\Sigma}$的$n$元正态分布$N(\mathbf{\mu, \Sigma}),\mathbf{A}$是$m$行$n$列的常数矩阵,则$m$维随机向量</li>
</ol>
<p>$$\mathbf{AX} \sim N(\mathbf{A\mu,A\Sigma A’})$$</p>
<ol start="4">
<li>显然$\frac{X_1-\mu}{\sigma},\frac{X_2-\mu}{\sigma},\cdots,\frac{X_n-\mu}{\sigma}$独立同$N(0,1)$分布,由$\chi^2$分布的定义可知它们的平方和</li>
</ol>
<p>$$\frac{1}{\sigma^2} \sum_{i=1}^{n}(X_1-\mu)^2 \sim \chi^2(n)$$</p>
<ol start="5">
<li>由独立正态变量的线性运算性质得</li>
</ol>
<p>$$\sum_{i=1}^nX_i \sim N(n\mu, n\sigma^2)$$</p>
<p>所以</p>
<p>$$\bar{X}=\frac{\displaystyle \sum_{i=1}^nX_i}{n} \sim N(\mu, \frac{\sigma^2}{n})$$</p>
<p>标准化得</p>
<p>$$\frac{\bar{X} - \mu}{\sigma / \sqrt{n}} \sim N(0,1)$$</p>
<ol start="6">
<li><p>$$\frac{1}{\sigma^2}\sum_{i=1}{n}(X_i-\bar{X})^2 \sim \chi^2(n-1)$$</p>
</li>
</ol>
<p>亦即</p>
<p>$$\frac{(n-1)S^2}{\sigma^2} \sim \chi^2(n-1)$$</p>
<ol start="7">
<li><p>$\frac{\sqrt{n}{\bar{X}-\mu}}{\sigma}$与$\frac{n-1)S^2}{\sigma^2}$相互独立.</p>
</li>
<li></li>
</ol>
<p>$$\frac{\sqrt{n}(\bar{x}-\mu)}{S} = \frac{\frac{\sqrt{n}(\bar{x}-\mu)}{\sigma}}{\sqrt{\frac{(n-1)S^2}{(n-1)\sigma^2}}} \sim t(n-1)$$</p>
<p>9.</p>
<p>$$\frac{(\bar{X}-\bar{Y})-(\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}} \sim N(0,1)$$</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://mejhwu.github.io/2018/04/19/summary_of_probability_theory/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="jhwu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/19/summary_of_probability_theory/" itemprop="url">
                  概率论总结
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-04-19 00:00:00" itemprop="dateCreated datePublished" datetime="2018-04-19T00:00:00+08:00">2018-04-19</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-06-27 20:18:19" itemprop="dateModified" datetime="2018-06-27T20:18:19+08:00">2018-06-27</time>
              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/数学/" itemprop="url" rel="index"><span itemprop="name">数学</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><h2 id="古典概型"><a href="#古典概型" class="headerlink" title="古典概型"></a>古典概型</h2><h3 id="古典改型的特点"><a href="#古典改型的特点" class="headerlink" title="古典改型的特点:"></a>古典改型的特点:</h3><p>(1) 试验的样本空间中的元素个数只有有限个,不妨设为n个,记为$e_1,e_2,…,e_n$;<br>(2) 每个基本事件${e}$出现的可能性相等,即有</p>
<p>$$P({e_1})=P({e_2})=…=P({e_n})$$</p>
<h3 id="古典概型类型"><a href="#古典概型类型" class="headerlink" title="古典概型类型"></a>古典概型类型</h3><h4 id="随机取数问题"><a href="#随机取数问题" class="headerlink" title="随机取数问题"></a>随机取数问题</h4><p>(1) 有放回地随机取数: 若从$n$个相异的数字中有放回地取$m$个, 则试验样本空间的基本事件总数可按$n$个不同数字中取$m$个的重复排列计算,由乘法原理知为$n<em>n</em>… *n=n^m$</p>
<p>(2) 无放回地随机取数: 若取出的数不还原,则从n个不同的数中任取$m$个试验样本空间所含基本事件总数要根据取数是计序或不计序,按不重复的排列或组合公式计算,即基本事件总数为:</p>
<p>(a) 计序时为$A_n^m=n(n-1)…(n-m+1)$</p>
<p>(b) 不计序时为$C_n^m= \left( \begin{matrix} n \ m \end{matrix} \right)=\frac{n!}{m!(n-m)!}$</p>
<h4 id="抽球问题"><a href="#抽球问题" class="headerlink" title="抽球问题"></a>抽球问题</h4><p>从$n$个球中抽取$m$个:</p>
<p>(1)有放回</p>
<p>(a)计序: $n^m$</p>
<p>(b)不计序: $C_{n+m-1}^m$, 相当于在$n+m-1$个球中无放回抽取$m$个球</p>
<p>(2)无放回</p>
<p>(a)计序: $A_n^m$</p>
<p>(b)不计序: $C_n^m$</p>
<h4 id="分房问题"><a href="#分房问题" class="headerlink" title="分房问题"></a>分房问题</h4><p>设有$n$个人,每个人都等可能的被分配到$N$个房间中的任意一间中去住$(n\le N)$, 且每个房间可容纳的人数不限.</p>
<h4 id="配对问题"><a href="#配对问题" class="headerlink" title="配对问题"></a>配对问题</h4><h2 id="几何概型"><a href="#几何概型" class="headerlink" title="几何概型"></a>几何概型</h2><p>(1) 每次试验的可能有无限多个,且全部可能结果的集合可用一个有度量(如长,面积,体积等)的几何区域来表示.</p>
<p>(2) 每次试验中每个可能的出现是等可能的.</p>
<h2 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h2><p>设$A, B$为两事件,且$P(A) \gt 0$, 称</p>
<p>$$P(B|A)=\frac{P(AB)}{P(A)}$$</p>
<p>为在事件$A$发生条件下事件$B$发生的概率.</p>
<p>全概率公式</p>
<p>设试验$E$的样本空间$S$, $A$为$E$的事件, $B_1,B_2,…,B_n$为$S$的一个划分,且$P(B_i)\gt 0(i=1,2,…,n)$, 则</p>
<p>$$P(A)=P(B_1)P(A|B_1)+P(B_2)P(A|B_2)+…+P(B_n)P(A|B_n)$$</p>
<p>贝叶斯公式</p>
<p>设试验$E$的样本空间$S$, $A$为$E$的事件, $B_1,B_2,…,B_n$为$S$的一个划分,且$P(A)&gt;0, P(B_i)\gt 0(i=1,2,…,n)$, 则</p>
<p>$$P(B_i|A)=\frac{P(B_i)P(A|B_i)}{\sum_{j=1}^nP(B_j)P(A|B_js)}$$</p>
<h2 id="事件独立"><a href="#事件独立" class="headerlink" title="事件独立"></a>事件独立</h2><p>设$A$,$B$是两事件,如果具有等式</p>
<p>$$P(AB)=P(A)P(B)$$</p>
<p>则称$A$,$B$为相互独立事件.</p>
<h1 id="随机变量"><a href="#随机变量" class="headerlink" title="随机变量"></a>随机变量</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>设$E$是随机试验, 其样本空间$S={e}$. 如果对应每一个$e \in S$, 均有一个实数$X(e)$与之对应, 这样一个定义在样本空间$S$上的单值实数$X=X(e)$, 称为随机变量.</p>
<p>设$X$是一个随机变量, $x$是任意实数, 函数</p>
<p>$$F(s)=P{X \le x}$$</p>
<p>称为$X$的分布函数.</p>
<p>分布函数的基本性质:</p>
<p>(1) $F(x)$是一个不减函数.</p>
<p>(2) $0 \le F(x) \le 1$ 且 $F(-\infty)= \lim_{x \rightarrow -\infty}F(x) = 0 \quad F(+\infty)= \lim_{x \rightarrow +\infty}F(x) = 1$</p>
<h2 id="离散型随机变量"><a href="#离散型随机变量" class="headerlink" title="离散型随机变量"></a>离散型随机变量</h2><p>若随机变量$X$的可能取值仅有有限个或可列多个,则称此随机变量为离散型随机变量.</p>
<h3 id="常见分布"><a href="#常见分布" class="headerlink" title="常见分布"></a>常见分布</h3><h4 id="单点分布"><a href="#单点分布" class="headerlink" title="单点分布"></a>单点分布</h4><p>设随机变量$X$取一个常数值$C$的概率为1, 即$P{X=C}=1$, 则称$X$服从单点分布或退化分布.</p>
<h4 id="0-1-分布-两点分布"><a href="#0-1-分布-两点分布" class="headerlink" title="(0-1)分布(两点分布)"></a>(0-1)分布(两点分布)</h4><p>设随机变量$X$只可能取0和1两个值,它的分布律是</p>
<p>$$P{X=k}=p^k(1-p)^{1-k} \qquad k=0,1; 0 &lt; p &lt; 1$$</p>
<p>则称$X$服从(0-1)分布.</p>
<h4 id="等可能分布-离散型均匀分布"><a href="#等可能分布-离散型均匀分布" class="headerlink" title="等可能分布(离散型均匀分布)"></a>等可能分布(离散型均匀分布)</h4><p>如果随机变量$X$可以取$n$个不同的值$x_1&lt;x_2&lt;…&lt;x_n$, 且取每个$x_k$值的概率相等, 即</p>
<p>$$P{X=x_k}=\frac {1}{n}$$</p>
<p>则称$X$服从等可能或称离散型均匀分布.</p>
<h4 id="二项分布"><a href="#二项分布" class="headerlink" title="二项分布"></a>二项分布</h4><p>如果随机变量$X$取值为$0,1,2,…,n$的概率为</p>
<p>$$P{X=k}= \left( \begin{matrix}  n \ k \end{matrix} \right) p^k (1-p)^{n-k}$$</p>
<p>则称$X$服从参数为$n,p$的二项分布, 记为$X \sim B(n,p)$</p>
<p>设试验$E$的可能结果只有两个,即$A$或$\bar{A}$, 且$P(A)=p, P(\bar{A})=1-p=q(0&lt;p&lt;1)$, 若将此试验$E$独立地重复$n$次,则称这一串重复的独立试验为$n$重贝努力(Bernoulli)试验,或称$n$重贝努力概型.</p>
<h4 id="泊松分布"><a href="#泊松分布" class="headerlink" title="泊松分布"></a>泊松分布</h4><p>如果随机变量X的可能取值为$0,1,2,…$取各值的概率为</p>
<p>$$P{X=k}=\frac{\lambda ^k e^{-\lambda}}{k!}$$</p>
<p>其中$\lambda&gt;0$为常数,则称$X$服从参数为$\lambda$的泊松分布,记为$X \sim P(\lambda)$.</p>
<p>泊松逼近定理</p>
<p>设$\lambda &gt; 0$是一常数, $n$是任意正整数, 设$np_n=\lambda$, 则对于任一固定的非负整数$k$, 有</p>
<p>$$\lim_{n\rightarrow \infty} \left( \begin{matrix}n \ p \end{matrix} \right) (1-p_n)^{n-k} = \frac{\lambda ^k e^{-\lambda}}{k!}$$</p>
<h4 id="几何分布"><a href="#几何分布" class="headerlink" title="几何分布"></a>几何分布</h4><p>如果随机变量$X$可能取值为$1,2,..$的概率为</p>
<p>$$P{X=k}=pq^{k-1} \qquad k=1,2,…, \quad 0 &lt; p &lt; 1, \quad q=1-p$$</p>
<p>则称$X$服从参数为$p$的几何分布,记为$X \sim Ge(p)$.</p>
<p>几何分布的数学描述:</p>
<p>若进行一系列重复的独立试验,每次试验中某事件$A$发生的概率为$p$,即$p=P(A)$, 令$X$表示事件$A$首次发送时试验的总次数,则此$X$服从参数为$p$的几何分布.</p>
<h4 id="帕斯卡分布-负二项分布"><a href="#帕斯卡分布-负二项分布" class="headerlink" title="帕斯卡分布(负二项分布)"></a>帕斯卡分布(负二项分布)</h4><p>如果随机变量$X$的概率分布为</p>
<p>$$P{X=k}=\left( \begin{matrix} k-1 \ r-1 \end{matrix} \right) p^r q^{k-r} $$</p>
<p>$$k=r,r+1,r+2,…,r \ge 1, \quad 0 &lt; p &lt; 1, \quad q = 1-p$$</p>
<p>则称$X$服从参数为$p,r$的帕斯卡分布或负二项分布.</p>
<p>帕斯卡分布的数学描述;</p>
<p>若进行一系列重复的独立试验, 每次试验中某事件$A$发生的概率为$p$发生的概率为$p$, 即$p=P(A)$.<br>令$X$表示在事件$A$恰发生$r$次时试验的总次数,则此$X$服从参数为$p,r$的帕斯卡分布.</p>
<h4 id="超几何分布"><a href="#超几何分布" class="headerlink" title="超几何分布"></a>超几何分布</h4><p>如果随机变量$X$的概率分布为</p>
<p>$$P{X=k}=\frac{\left( \begin{matrix} M \ k \end{matrix} \right) \left( \begin{matrix} N-M \ n-k \end{matrix} \right) }{\left( \begin{matrix} N \ n \end{matrix} \right)}$$</p>
<p>则称$X$服从参数为$n,M,N$的超几何分布.</p>
<p>超几何分布的数学描述:</p>
<p>设一代中总有$N$个产品,其中有$M$个次品,现从中任取$n$个产品,令$X$为$n$个产品中的次品的个数,则此$X$服从参数为$n,M,N$的超几何分布.</p>
<p>定理:</p>
<p>设$p(0&lt;p&lt;1)$为一常数,当$N \rightarrow \infty$时, $\lim \frac{M}{N} = p$, 则对固定的非负整数$n(n\le M)$, 任一固定的非负整数$k=0,1,2,…,n$ 有</p>
<p>$$\lim_{N \rightarrow \infty} \frac {\left( \begin{matrix} M \ k \end{matrix} \right) \left( \begin{matrix} N-M \ n-k \end{matrix} \right)}{\left( \begin{matrix} N \ n \end{matrix} \right)} = \left( \begin{matrix} n \ k \end{matrix} \right)p^k(1-p)^{n-k}$$</p>
<h2 id="连续型随机变量"><a href="#连续型随机变量" class="headerlink" title="连续型随机变量"></a>连续型随机变量</h2><h3 id="概念-1"><a href="#概念-1" class="headerlink" title="概念"></a>概念</h3><p>如果对于随机变量$X$的分布函数$F(x)$, 存在非负函数$f(x)$, 使对于任意实数$x$均有</p>
<p>$$F(x)=\int^x_{-\infty} f(t)dt $$</p>
<p>则称$X$为连续型随机变量, 其中函数$f(x)$称为$X$的概率密度函数,简称概率密度.</p>
<p>概率密度$f(x)$具有以下性质:</p>
<p>(1) $f(x) \ge 0$</p>
<p>(2) $\int_{- \infty}^{+ \infty}f(x)dx=1$</p>
<p>(3) $P{x_1 &lt; X \le x_2} = F(x_2) - F(x_1) = \int_{x_1}^{x_2}dx \qquad (x_1 \le x_2$)</p>
<p>(4) 若$f(x)$在点$x$处连续, 则有$F^{‘}(x) = f(x)$</p>
<p>对于连续型随机变量$X$来说,$X$取任一固定值$a$的概率为0, 故连续型随机变量概率与区间开闭无关:</p>
<p>$$P{a \le x \le b} = P{a \le x &lt; b} = P{a &lt; x \le b} = P{a &lt; x &lt; b}$$</p>
<h3 id="连续型随机变量的分布"><a href="#连续型随机变量的分布" class="headerlink" title="连续型随机变量的分布"></a>连续型随机变量的分布</h3><h4 id="均匀分布"><a href="#均匀分布" class="headerlink" title="均匀分布"></a>均匀分布</h4><p>若随机变量$X$具有概率密度</p>
<p>$$f(x)=\begin{cases} &amp; \frac{1}{b-a} \quad &amp; a &lt; x &lt; b \ &amp; 1 \quad &amp; x \ge b, x \le a \end{cases}$$</p>
<p>s则称$X$服从$(a,b)$上的均匀分布, 记作$X \sim U(a,b)$. 当参数$a=0, b=1$时, $U(a,b)$成为标准分布.</p>
<p>容易得到$X$的分布函数为</p>
<p>$$f(x)=\begin{cases} 0 &amp; x&lt;a \ \frac{x-a}{b-a} &amp; a \le x &lt; b \ 1 &amp; x \ge b \end{cases}$$</p>
<h4 id="指数分布"><a href="#指数分布" class="headerlink" title="指数分布"></a>指数分布</h4><p>若随机变量$X$具有概率密度</p>
<p>$$f(x)=\begin{cases} \alpha e^{-\alpha x} &amp; x&gt;0 \ 0 &amp; x \le 0 \end{cases}$$</p>
<p>其中参数$\alpha &gt; 0$, 则称随机变量$X$服从参数为$\alpha$的指数分布, 记为$X \sim Z(\alpha)$.</p>
<p>其分布函数为</p>
<p>$$F(x)=\begin{cases} 1-e^{-\alpha x} &amp; x &gt; 0 \ 0 &amp; x \le 0 \end{cases}$$</p>
<h4 id="正态分布"><a href="#正态分布" class="headerlink" title="正态分布"></a>正态分布</h4><p>若随机变量$X$具有概率密度</p>
<p>$$f(x)=\frac{1}{\sqrt{2 \pi \sigma}} e^{-\frac{(x-\mu)^2}{2 \sigma^2}}$$</p>
<p>则称$X$服从参数为$\mu$和$\sigma^2(\sigma &gt; 0)$的正态分布,记作$X \sim N(\mu, \sigma^2)$, 此时称$X$为正态变量.</p>
<p>$X$的分布函数为</p>
<p>$$F(x)=\int _{-\infty}^x \frac{1}{\sqrt{2 \pi \sigma}} e^{-\frac{(t-\mu)^2}{2 \sigma^2}} dt \qquad -\infty &lt; x &lt; +\infty$$</p>
<p>当$\mu =0, \sigma = 1$时, $X \sim N(0,1)$, 称$X$服从标准正态分布,称$X$为标准正态变量. 它的概率密度和分布函数分布标记为</p>
<p>$$\varphi(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \qquad -\infty&lt;x&lt;+\infty $$</p>
<p>$$\Phi(x)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x}e^{-\frac{t^2}{2}}dt \qquad -\infty&lt;x&lt;+\infty $$</p>
<p>若$X \sim N(\mu, \sigma^2)$, 其分布函数为$F(x)$, 则标准化变量</p>
<p>$$Z=\frac{X-\mu}{\sigma} \sim N(0,1)$$</p>
<p>且</p>
<p>$$F(x)=\Phi(\frac{x-\mu}{\sigma})$$</p>
<p>设随机变量$X$的分布函数$F(x)$,对于任一正数$\alpha(0&lt;\alpha&lt;1)$, 若$X$大于等于某实数$z_\alpha$, 即</p>
<p>$$1-F(z_\alpha)=P{X \ge z_\alpha} = \alpha \qquad 0 &lt; \alpha &lt; 1$$</p>
<p>则称此实数$z_\alpha$为分布$F(x)$的上$\alpha$分为点.</p>
<h2 id="随机变量的函数的分布"><a href="#随机变量的函数的分布" class="headerlink" title="随机变量的函数的分布"></a>随机变量的函数的分布</h2><h3 id="离散型随机变量的函数的分布"><a href="#离散型随机变量的函数的分布" class="headerlink" title="离散型随机变量的函数的分布"></a>离散型随机变量的函数的分布</h3><p>离散型随机变量的函数$Y=g(X)$仍是离散型随机变量.计算离散型随机变量的函数的分布律,首先找出它的一切可能值,然后计算它取各个值的概率.</p>
<h3 id="连续型随机变量的函数的分布"><a href="#连续型随机变量的函数的分布" class="headerlink" title="连续型随机变量的函数的分布"></a>连续型随机变量的函数的分布</h3><p>如果$X$是连续型随机变量,函数$g(x)$是连续函数,这时$Y=g(X)$也是一个连续型随机变量.</p>
<p>设连续型随机变量$X$的概率密度为$f_X(s)$, 当$a&lt;x&lt;b$时, $f_X(x)&gt;0;y=g(x)$处处可导, 且恒有$g’(x)&gt;0$(或$g’(x)&lt;0$),则随机变量$X$的函数$Y=g(X)$的概率密度为</p>
<p>$$f_Y(y)=\begin{cases} f_X(h(y))|h’(y)| &amp; c&lt;y&lt;d \ 0 &amp; y\ge d, y &lt; c \end{cases}$$</p>
<p>其中$x=h(y)$为$y=g(x)$的反函数, $c=min{g(a), g(b)}, d=max{g(a),g(b)}$</p>
<p>设随机变量$X \sim N(\mu, \sigma^2)$, $X$的线性函数$Y=aX+b(a\neq 0)$也服从正态分布.</p>
<h1 id="多维随机变量"><a href="#多维随机变量" class="headerlink" title="多维随机变量"></a>多维随机变量</h1><h2 id="二维随机变量"><a href="#二维随机变量" class="headerlink" title="二维随机变量"></a>二维随机变量</h2><p>设$E$为一个随机试验,它的样本空间为$S={e}$, 并设$X=X(e)$和$Y=Y(e)$是定义在$S$上的随机变量,由它们两个构成的联合变量$(X,Y)$,称为二维随机变量或二维随机向量.</p>
<p>设$(X,Y)$是定义在样本空间$S={e}$上的二维随机变量, 对于任意实数$x,y$, 二元函数</p>
<p>$$F(x,y)=P{(X\le x) \cap (Y\le y)} \triangleq P{X\le x, Y\le y}$$</p>
<p>称为二维随机变量$(X,Y)$的分布函数, 或称为随机变量$X$和$Y$的联合分布函数.</p>
<p>二维随机变量的分布函数的性质:</p>
<p>(1) $F(x,y)$是变量$x$和$y$的不减函数.</p>
<p>(2) $0 \le F(x,y) \le 1$, 且对于任意固定的$y, F(-\infty,y)=0$;对于固定的$x$,有$F(x,-\infty)=0$; 且$F(-\infty, +\infty)=0;F(+\infty,-\infty)=1$.</p>
<p>(3) $F(x,y)=F(x+0,y)=F(x, y+0)$, 即$F(x,y)$关于$x$右连续,关于$y$右连续.</p>
<p>(4) 对于任意的$x_1&lt;x_2, y_1&lt;y_2$, 下述不等式成立</p>
<p>$$F(x_2,y_2)-F(x_2,y_1)-F(x_1,y_2)-F(x_1,y_1) \ge 0$$</p>
<p>$X,Y$的边缘分布函数$F_X(x),F_Y(y)$:</p>
<p>$$F_X(x)=P{X \le x} = P{X \le x, Y &lt; +\infty}=F(x,+\infty)$$</p>
<p>$$F_Y(y)=P{Y \le y} = P{X &lt; +\infty, Y \le y}=F(+\infty, y)$$</p>
<h3 id="二维离散型随机变量"><a href="#二维离散型随机变量" class="headerlink" title="二维离散型随机变量"></a>二维离散型随机变量</h3><p>如果二维随机变量$(X,Y)$的所有可能取的值是有限对或可列多对,则称$(X,Y)$是二维离散想随机变量.</p>
<p>设二维离散型随机变量$(X,Y)$所有可能取的值为$(x_i,y_j)(i,j=1,2,…)$, 其概率记为$P{X=x_i,Y=y_j} = p_{ij}(i,j=1,2,…)$,则由概率的定义有</p>
<p>(1) $p_{ij} \ge 0$</p>
<p>(2) $\sum_{i=1}^\infty \sum_{j=1}^\infty p_{ij} = 1$</p>
<p>$(X,Y)$的分布函数为</p>
<p>$$F(x,y)=P{X \le x, Y\le y}=\sum_{x_i\le x}\sum_{y_j\le y}P{X=x_i, Y=y_j}=\sum_{x_i\le x}\sum_{y_j\le y}p_{ij}$$</p>
<p>$X$的分布律:</p>
<p>$$P{X=x_i}=\sum_{j=1}^\infty p_{ij} \triangleq p_{i\cdot} \qquad i=1,2,…$$</p>
<p>$Y$的分布律:</p>
<p>$$P{Y=y_j}=\sum_{i=1}^\infty p_{ij} \triangleq p_{\cdot i} \qquad j=1,2,…$$</p>
<h3 id="二维连续型随机变量"><a href="#二维连续型随机变量" class="headerlink" title="二维连续型随机变量"></a>二维连续型随机变量</h3><p>如果二维随机变量$(X,Y)$的分布函数$F(x,y)$, 存在一个非负可积的二元函数$f(x,y)$,使它对应任意实数$x,y$, 都有</p>
<p>$$F(x,y)=\int_{-\infty}^x \int_{-\infty}^y f(s,t)dsdt$$</p>
<p>则称$(X,Y)$是二维连续型随机变量,函数$f(x,y)$称为二维随机变量$(X,Y)$的概率密度,或称为随机变量$X$和$Y$的联合概率密度.</p>
<p>概率密度$f(x,y)$具有以下性质:</p>
<p>(1) $f(x,y) \ge 0$</p>
<p>(2) $\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} f(x,y)dxdy = 1$</p>
<p>(3) 若$f(x,y)$在点$(x,y)$连续, 则有</p>
<p>$$\frac{\partial^2 F(x,y)}{\partial x \partial y}=f(x,y)$$</p>
<p>(4) 随机点$(X,Y)$落在平面区域$D$上的概率为</p>
<p>$$P{(X,Y) \in D}=\int \int_D f(x,y) dxdy$$</p>
<p>$X$为连续型变量,其概率密度为</p>
<p>$$f_X(x)=\int_{-\infty}^{+\infty}f(x,y)dy$$</p>
<p>$Y$为连续型变量,其概率密度为</p>
<p>$$f_Y(y)=\int_{-\infty}^{+\infty}f(x,y)dx$$</p>
<p>$X,Y$的联合分布可以确定$X,Y$的边缘分布函数, 反过来,有$X$和$Y$的边缘分布,一般不能确定$X$和$Y$的联合分布.</p>
<h2 id="条件分布"><a href="#条件分布" class="headerlink" title="条件分布"></a>条件分布</h2><h3 id="条件分布律"><a href="#条件分布律" class="headerlink" title="条件分布律"></a>条件分布律</h3><p>设$(X,Y)$是离散型随机变量, 可能取值为$(x_i,y_j)(i,j=1,2,…)$, 其分布律及边缘分布律分别为$P{X=x_i,Y=y_j}=p_{ij}, P{X=x_i}=p_{i\cdot}, P{Y=y_j}=p_{\cdot j}$.</p>
<p>若对于固定的$i, P{X=x_i} &gt; 0$,则称</p>
<p>$$P{Y=y_j|X=x_i}=\frac{P{X=x_i,Y=y_j}}{P{X=x_i}}=\frac{p_{ij}}{p_{i\cdot}} \triangleq p_{j|i}$$</p>
<p>为在$X=x_i$条件下$Y$的条件分布律.</p>
<p>若对于固定的$j, P{Y=y_j}&gt;0$,则称</p>
<p>$$P{X=x_i|Y=y_j}=\frac{P{X=x_i,Y=y_j}}{P{Y=y_j}}=\frac{p_{ij}}{p_{\cdot j}} \triangleq p_{i|j}$$</p>
<p>为在$Y=y_j$条件下$X$的条件分布律.</p>
<h3 id="条件分布函数与条件概率密度"><a href="#条件分布函数与条件概率密度" class="headerlink" title="条件分布函数与条件概率密度"></a>条件分布函数与条件概率密度</h3><p>由离散型分布函数定义可得条件分布函数为</p>
<p>$$F_{Y|X}(y|x_i)=P{Y \le y | X=x_i}=\sum_{y_j\le y} p_{j|i}$$</p>
<p>$$F_{X|Y}(x|y_j)=P{X \le x | Y=y_j}=\sum_{x_i\le x} p_{i|j}$$</p>
<p>给定$y$, 设对于任意固定的正数$\varepsilon, P{y-\varepsilon &lt; Y \le y+\varepsilon } &gt; 0$, 且若对于任意实数$x$,极限</p>
<p>$$\lim_{\varepsilon \rightarrow 0^+} P{X \le x | y-\varepsilon &lt; Y \le y+\varepsilon } = \lim_{\varepsilon \rightarrow 0^+} \frac{P{X \le x , y-\varepsilon &lt; Y \le y+\varepsilon }}{P{y-\varepsilon &lt; Y \le y+\varepsilon }}$$</p>
<p>存在,则称此极限在条件$Y=y$下$X$的条件分布函数,记为$P{X \le x | Y=y}$, 或记为$F_{X|Y}(x|y)$.s</p>
<p>设$(X,Y)$的分布函数为$F(x,y)$,概率密度为$f(x,y)$, 若在点$(x,y)$处$f(x,y)$连续,边缘概率密度$f_Y(y)$连续,且$f_Y(y) &gt; 0$,则有</p>
<p>$$F_{X|Y}(x|y)=\int_{-\infty}^x \frac{f(u,y)}{f_Y(y)} du$$</p>
<p>$$f_{X|Y}(x|y)=\frac{f(x,y)}{f_Y(y)}$$</p>
<p>同理</p>
<p>$$F_{Y|X}(y|x)=\int_{-\infty}^y \frac{f(x,v)}{f_X(x)} dv$$</p>
<p>$$f_{Y|X}(y|x)=\frac{f(x,y)}{f_X(x)}$$</p>
<h2 id="相互独立的随机变量"><a href="#相互独立的随机变量" class="headerlink" title="相互独立的随机变量"></a>相互独立的随机变量</h2><p>设$F(x,y)$及$F_X(x),F_Y(y)$分别是二维随机变量$(X,Y)$的分布函数及边缘分布函数,若对于所有$x,y$有</p>
<p>$$P{X\le x, Y\le y}=P{X\le x}P{Y\le y}$$</p>
<p>即$F(x,y)=F_X(x)F_Y(y)$, 则称随机变量$X$和$Y$是相互独立的.</p>
<h3 id="离散型随机变量-1"><a href="#离散型随机变量-1" class="headerlink" title="离散型随机变量"></a>离散型随机变量</h3><p>离散型随机变量$X$和$Y$相互独立的充要条件是它们的联合分布函数等于两个边缘分布绿的乘积,即</p>
<p>$$P{X=x_i,Y=y_j}=P{X=x_i}P{Y=y_j}$$<br>即$p_{ij}=p_{i\cdot} \cdot p_{\cdot j}$ </p>
<h3 id="连续型随机变量-1"><a href="#连续型随机变量-1" class="headerlink" title="连续型随机变量"></a>连续型随机变量</h3><p>连续型随机变量$X$和$Y$相互独立的充要条件是它们的联合概率密度$f(x,y)$等于边缘概率密度$f_X(x)$和$f_Y(y)$的乘积,即</p>
<p>$$f(x,y)=f_X(x)f_Y(y)$$</p>
<h2 id="两个随机变量的函数的分布"><a href="#两个随机变量的函数的分布" class="headerlink" title="两个随机变量的函数的分布"></a>两个随机变量的函数的分布</h2><h3 id="两个离散型随机变量的函数的分布"><a href="#两个离散型随机变量的函数的分布" class="headerlink" title="两个离散型随机变量的函数的分布"></a>两个离散型随机变量的函数的分布</h3><p>$Z=g(X,Y)$的概率分布的一般求法</p>
<p>设$(X,Y)$为离散型随机变量,其概率分布为</p>
<p>$$P{X=x_i,Y=y_j} = p_{ij} \qquad i,j=1,2,…$$</p>
<p>则$Z=g(X,Y)$的概率分布的一般求法是:先确定函数$Z=g(X,Y)$的全部可能取值$z=g(x_i,y_j)(i,j=1,2,…)$;再确定相应概率</p>
<p>$$P{Z=g(x_i,y_j)}=P{X=x_i,Y=y_j}=p_{ij}$$</p>
<p>然后将$z=g(x_i,y_j)(i,j=1,2,…)$中相同的值合并,相应的概率相加,并将$z$值按从小到大的顺序重新排列,且与其概率对应,即可写出$Z=g(X,Y)$的概率分布.</p>
<p>两个离散型随机变量的和的概率公式</p>
<p>$$P{Z=k}=\sum_{i=0}^k P{X=i}P{Y=k-i}=\sum_{j=0}^k P{Y=j}P{X=k-j}$$</p>
<h3 id="两个连续型随机变量的函数的分布"><a href="#两个连续型随机变量的函数的分布" class="headerlink" title="两个连续型随机变量的函数的分布"></a>两个连续型随机变量的函数的分布</h3><h4 id="随机变量的和的分布"><a href="#随机变量的和的分布" class="headerlink" title="随机变量的和的分布"></a>随机变量的和的分布</h4><p>$$f_Z(z)=\int_{-\infty}^{+\infty}f(z-y,y)dy$$</p>
<p>或</p>
<p>$$f_Z(z)=\int_{-\infty}^{+\infty}f(x,z-x)dx$$</p>
<h4 id="随机变量的商的分布"><a href="#随机变量的商的分布" class="headerlink" title="随机变量的商的分布"></a>随机变量的商的分布</h4><p>$$f_Z(z)=\int_{-\infty}^{+\infty}|y|f(yz,y)dy$$</p>
<h4 id="随机变量的极值的分布"><a href="#随机变量的极值的分布" class="headerlink" title="随机变量的极值的分布"></a>随机变量的极值的分布</h4><p>设$X,Y$是两个相互独立的随机事件,称$M=max(X,Y)$为最大值变量, $N=min(X,Y)$为最小值变量, 统称为极值变量.</p>
<p>$$F_{max}(z)=P{M \le z}=P{X\le z}P{Y\le z}=F_X(z)F_Y(z)$$</p>
<p>$$\begin{aligned} F_{min}(z) &amp;=P{N\le z} \ &amp;=1-P{N&gt;z} \ &amp;=1-P{Y&gt;z,X&gt;z} \ &amp;=1-P{X&gt;z}P{Y&gt;z} \ &amp;=1-[1-F_X(z)][1-F_Y(z)] \end{aligned} $$</p>
<h2 id="n-N-ge-2-维随机变量"><a href="#n-N-ge-2-维随机变量" class="headerlink" title="$n(N\ge 2)$维随机变量"></a>$n(N\ge 2)$维随机变量</h2><h3 id="n-N-ge-2-维随机变量及其分布"><a href="#n-N-ge-2-维随机变量及其分布" class="headerlink" title="$n(N\ge 2)$维随机变量及其分布"></a>$n(N\ge 2)$维随机变量及其分布</h3><p>设$E$是一个随机变量,其样本空间为$S={e}$, 设$X_i=X_i(e),(i=1,2,…,n)$是定义在$S$上的$n$个随机变量,由它们构成的一个向量</p>
<p>$$(X_1,X_2,…,X_n)=(X_1(e),X_2(e),…X_n(e)) \qquad e \in S$$</p>
<p>称为$n$维随机向量或$n$维随机变量.</p>
<p>设$(X_1,X_2,…,X_n)$是$n$维随机变量,对于任意实数$x_1,x_2,…,x_n$,$n$元函数</p>
<p>$$F(x_1,x_2,…,x_n)=P{X_1\le x_1, X_2\le x_2, …, X_n \le x_n}$$</p>
<p>称为$n$维随机变量$(X_1,X_2,…,X_n)$的分布函数,或称为随机变量$X_1,X_2,…X_n$的联合分布函数.</p>
<p>若存在非负函数$f(x_1,x_2,…x_n)$, 使对于任意实数$x_1,x_2,…,x_n$,有</p>
<p>$$F(x_1,x_2,…,x_n)=\int_{-\infty}^{x_n}\int_{-\infty}^{x_{n-1}}…\int_{-\infty}^{x_1}f(x_1,x_2,…,x_n)dx_1dx_2…dx_n$$</p>
<p>则称$f(x_1,x_2,…x_n)$为$n$维连续型随机变量$(X_1,X_2,…,X_n)$的概率密度函数.</p>
<p>设$X_i$可能取值为$x_{ij_i}(i=1,2,…,n, \quad j_i=1,2,…)$, 则记</p>
<p>$$P{X_1=x_{1j_1},X_2=x_{2j_2},…,X_n=x_{nj_n}}=p_{j_1j_2…j_ns}$$</p>
<p>为$n$为离散型随机变量$(X_1,X_2,…,X_n)$的概率分布或分布律,或随机变量$(X_1,X_2,…,X_n)$的联合分布律.</p>
<p>对于$n$维空间中任一区域$D, {X_1,X_2,…,X_n} \in D$ 为一随机事件,若$X_1,X_2,…,X_n$具有概率密度$f(x_1,x_2,…,x_n$, 则概率</p>
<p>$$P{(X_1,X_2,…,X_n) \in D}=\int \int … \int _D f(x_1,x_2,…,x_n)dx_1dx_2…dx_n$$</p>
<h3 id="n-个随机变量的相互独立性"><a href="#n-个随机变量的相互独立性" class="headerlink" title="$n$个随机变量的相互独立性"></a>$n$个随机变量的相互独立性</h3><p>若对于所有实数$x_1,x_2,…,x_n$有</p>
<p>$$F(x_1,x_2,…,x_n)=F_{X_1}(x_1)F_{X_2}(x_2)…F_{X_n}(x_n)$$</p>
<p>则称$n$个随机变量$X_1,X_2,…,X_n$是相互独立的.</p>
<p>并且</p>
<p>$$f(x_1,x_2,…,x_n)=f_{X_1}(x_1)f_{X_2}(x_2)…f_{X_n}(x_n)$$</p>
<p>若对所有的实数$x_1,x_2,…,x_m,y_1,y_2,…,y_n$,随机变量$(X_1,X_2,…,X_n,Y_1,Y_2,…,Y_n)$的分布函数$F(x_1,x_2,…,x_m,y_1,y_2,…,y_n)$与关于$(X_1,X_2,…,X_m)$及$(Y_1,Y_2,…,Y_n)$的边缘分布函数$F_1(x_1,x_2,…,x_m), F_2(y_1,y_2,…,y_n)$满足下述等式:</p>
<p>$$F(x_1,x_2,…,x_m,y_1,y_2,…,y_n)=F_1(x_1,x_2,…,x_m)F_2(y_1,y_2,…,y_n)$$</p>
<p>则称随机变量$(X_1,X_2,…,X_m)$与$(Y_1,Y_2,…,Y_n)$是相互独立的.</p>
<p>设$(X_1,X_2,…,X_m)$和$(Y_1,Y_2,…,Y_n)$相互独立,则$X_i(i=1,2,…,m)$与$Y_j(j=1,2,…,n)$相互独立.又若$h,g$是连续函数,则$h(X_1,X_2,…,X_m)$与$g(Y_1,Y_2,…,Y_n)$相互独立.</p>
<h3 id="n-维随机变量的函数的分布"><a href="#n-维随机变量的函数的分布" class="headerlink" title="$n$维随机变量的函数的分布"></a>$n$维随机变量的函数的分布</h3><p>设$(X_1,X_2,…,X_n)$为$n$维随机变量,其概率密度为$f(x_1,x_2,…,x_n)$,$g(x_1,x_2,…,x_n)$为$n$元连续函数,则对任意实数$z \in R, Z=g(X_1,X_2,…,X_n)$的分布函数为</p>
<p>$$F_Z(z)=P{g(X_1,X_2,…,X_n) \le z}=\int \int … \int_{D_Z}f(x_1,x_2,…,x_n)dx_1dx_2…dx_n$$</p>
<p>其中$D_Z={(x_1,x_2,…,x_n)|g(x_1,x_2,…,x_n) \le z}$.</p>
<h1 id="随机变量的数字特征"><a href="#随机变量的数字特征" class="headerlink" title="随机变量的数字特征"></a>随机变量的数字特征</h1><h2 id="数学期望"><a href="#数学期望" class="headerlink" title="数学期望"></a>数学期望</h2><h3 id="数学期望的概念"><a href="#数学期望的概念" class="headerlink" title="数学期望的概念"></a>数学期望的概念</h3><p>设离散型随机变量$X$的分布律为</p>
<p>$$P{X=x_k}=p_k \qquad k=1,2,…$$</p>
<p>若级数$\sum_{k=1}^{\infty}x_kp_k$绝对收敛,则称级数$\sum_{k=1}^{\infty}x_kp_ks$的值为离散型随机变量$X$的数学期望,记为$E(X)$, 即</p>
<p>$$E(X)=\sum_{k=1}^{\infty}x_kp_k$$</p>
<p>数学期望可简称为期望或均值.</p>
<p>设连续型随机变量$X$的概率密度为$f(x)$,若积分</p>
<p>$$f_{-\infty}^{+\infty}xf(x)dx$$</p>
<p>绝对收敛,则称积分$f_{-\infty}^{+\infty}xf(x)dx$的值为随机变量$X$的数学期望,记为$E(X)$,即</p>
<p>$$E(X)=f_{-\infty}^{+\infty}xf(x)dx$$</p>
<h3 id="随机变量的函数的数学期望"><a href="#随机变量的函数的数学期望" class="headerlink" title="随机变量的函数的数学期望"></a>随机变量的函数的数学期望</h3><p>设$Y$是随机变量$X$的函数:$Y=g(X)$($g$是连续函数)</p>
<ol>
<li>$X$是离散型随机变量,它的分布律为$p_K=P{X=x_k}(k=1,2,…)$,若$\sum_{k=1}^{\infty}g(x_k)p_k$绝对收敛,则有</li>
</ol>
<p>$$E(Y)=E[g(X)]=\sum_{k=1}^{\infty}g(x_k)p_k$$</p>
<ol start="2">
<li>$X$是连续型随机变量,它的概率密度为$f(x)$,若$\int_{-\infty}^{+\infty}g(x)f(x)dx$绝对收敛,则有</li>
</ol>
<p>$$E(Y)=E[g(X)]=\int_{-\infty}^{+\infty}g(x)f(x)dx$$</p>
<h3 id="数学期望的简单性质"><a href="#数学期望的简单性质" class="headerlink" title="数学期望的简单性质"></a>数学期望的简单性质</h3><ol>
<li>(线性法则)设$X$为随机变量,其期望为$E(X)$,对于任意常数$a,b$有</li>
</ol>
<p>$$E(aX+b)=aE(X)+b$$</p>
<ol start="2">
<li>(加法法则) 设$X,Y$为随机变量,则有</li>
</ol>
<p>$$E(X+Y)=E(X)+E(Y)$$</p>
<ol start="3">
<li>(乘法法则) 设$X,Y$为两个相互独立的随机变量,则</li>
</ol>
<p>$$E(XY)=E(X)E(Y)$$</p>
<ol start="4">
<li><p>(柯西-许瓦兹不等式)</p>
<p> 设$X$与$Y$是两个随机变量,则</p>
</li>
</ol>
<p>$$|E(XY)|^2 \le E(X^2)E(Y^2)$$</p>
<h2 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h2><h3 id="概念-2"><a href="#概念-2" class="headerlink" title="概念"></a>概念</h3><p>设$X$是一个随机变量,若$E{[X-E(X)]^2}$存在,则乘$E{[X-E(X)]^2}$称为$X$的方差,记为$D(X)$或$Var(X)$,即</p>
<p>$$D(X)=Var(X)=E{[X-E(X)]^2}$$</p>
<p>显然$D(X) \ge 0$, 故在应用中引入与随机变量$X$具有相同量纲的量$\sqrt{D(X)}$, 记为$\sigma(X)$,称为标准差或均方差.</p>
<p>由方差的定义可知,随机变量$X$的方差实际上就是$X$的函数$Y=g(X)=(X-E(X))^2$. 故</p>
<p>若$X$为离散型随机变量,其分布律为$p_k=P{X=x_k}(k=1,2,…)$,其方差为</p>
<p>$$D(X)=\sum_{k=1}^{\infty}[x_k-E(X)]^2p_k$$</p>
<p>若$X$为连续型随机变量,其概率密度为$f(x)$,则$X$的方差为</p>
<p>$$D(X)=\int_{-\infty}^{+\infty}[x-E(X)]^2f(x)dx$$</p>
<p>并且具有公式</p>
<p>$$D(X)=E[X^2]-[E(X)]^2$$</p>
<h3 id="方差的简单性质"><a href="#方差的简单性质" class="headerlink" title="方差的简单性质"></a>方差的简单性质</h3><ol>
<li>设$X$为随机变量,对于任意的常数$a,b$</li>
</ol>
<p>$$D(aX+b)=a^2D(X)$$</p>
<ol start="2">
<li>设$X,Y$为两个相互独立的随机变量,则有</li>
</ol>
<p>$$D(X+Y)=D(X)+D(Y)$$</p>
<ol start="3">
<li>(契比雪夫不等式) 设$X$为一随机变量,其均值$E(X)=\mu$,方差$D(X)=\sigma^2$,则对任意正数$\sigma &gt; 0$,有</li>
</ol>
<p>$$P{|X-\mu|\ge \varepsilon } \le \frac{\sigma^2}{\varepsilon^2}$$</p>
<ol start="4">
<li>$D(X)=0$的充要条件是$X$以概率1取常数$\mu =E(X)$,即</li>
</ol>
<p>$$P{X=\mu } = 1$$</p>
<h3 id="几种重要随机变量的数学期望及方差"><a href="#几种重要随机变量的数学期望及方差" class="headerlink" title="几种重要随机变量的数学期望及方差"></a>几种重要随机变量的数学期望及方差</h3><ol>
<li><p>二项分布$B(n,p)$</p>
<p> 设$X$服从参数为$n,p$的二项分布,其分布律为:</p>
</li>
</ol>
<p>$$P{X=k}=\left( \begin{matrix} n \ k \end{matrix} \right) p^k(1-p)^{n-k}$$</p>
<p>$E(X)=np, \quad D(X)=np(1-p)$</p>
<ol start="2">
<li><p>泊松分布$\pi(\lambda)$</p>
<p> 设$X$服从参数为$\lambda$的泊松分布,其 分布律为:</p>
</li>
</ol>
<p>$$P{X=k}=\frac{\lambda^ke^{-\lambda}}{k!} \qquad k=0,1,2,…; \lambda&gt;0$$</p>
<p>$E(X)=\lambda \qquad D(X)=\lambda$</p>
<ol start="3">
<li><p>几何分布$Ge(p)$</p>
<p> 设$X$服从参数为$p$的几何分布,其分布律为</p>
</li>
</ol>
<p>$$P{X=k}=pq^{k-1} \quad k=1,2,.., \quad 0&lt;p&lt;1, q=1-p$$</p>
<p>$E(X)=\frac{1}{p} \qquad D(X)=\frac{q}{p^2}$</p>
<ol start="4">
<li>均匀分布$U(a,b)$</li>
</ol>
<p>设在区间$(a,b)$上服从均匀分布, 其概率密度为</p>
<p>$$f(x)=\begin{cases} \frac{1}{b-q} &amp; a&lt;x&lt;b \ 0 &amp; other \end{cases}$$</p>
<p>$E(X)=\frac{a+b}{2} \qquad D(x)=\frac{(b-a)^2}{12}$</p>
<ol start="5">
<li><p>指数分布$Z(\alpha)$</p>
<p> 设$X$服从参数为$\alpha$的指数分布,其概率密度为</p>
</li>
</ol>
<p>$$f(x)=\begin{cases} \alpha e^{-\alpha x} &amp; x&gt;0, \alpha &gt; 0 \ 0 &amp; x \le 0 \end{cases}$$</p>
<p>$E(X)=\frac{1}{\alpha} \qquad D(X)=\frac{1}{\alpha ^2}$</p>
<ol start="6">
<li><p>正态分布$N(\mu, \sigma ^2)$</p>
<p> 设$X$服从参数为$\mu, \sigma ^2$的正态分布,其概率密度为</p>
</li>
</ol>
<p>$$f(X)=\frac{1}{\sqrt{2\pi \sigma}} e ^{-\frac{(x-\mu)^2}{2\sigma ^2}} \qquad \sigma &gt; 0, -\infty &lt; x &lt; +\infty$$</p>
<p>$E(X)=\mu, \qquad D(X)=\sigma ^2$</p>
<h2 id="协方差与相关系数"><a href="#协方差与相关系数" class="headerlink" title="协方差与相关系数"></a>协方差与相关系数</h2><h3 id="概念-3"><a href="#概念-3" class="headerlink" title="概念"></a>概念</h3><p>设$(X,Y)$为二维随机变量,量$E{[X-E(X)][Y-E(Y)]}$称为$X$与$Y$的协方差, 记为$Cov(X,Y)$,即</p>
<p>$$Cov(X,Y)=E{[X-E(X)][Y-E(Y)]}$$</p>
<p>而量</p>
<p>$$\rho _{_{XY}}=\frac{Cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}$$</p>
<p>称为随机变量$X$与$Y$的相关系数, $\rho _{_{XY}}$是一个无量纲的量.</p>
<p>特别的,当$Y=X$时, $Cov(X,X)=D(X)$,此时$\rho _{_{XY}}=1$. 并且</p>
<p>$$D(X+Y)=D(X)+D(Y)+2Cov(X,Y)$$</p>
<p>$$Cov(X,Y)=E(XY)-E(X)E(Y)$$</p>
<p>设$X,Y$为随机变量,$a,b$为任意常数,则协方差$Cov(X,Y)$具有一下性质:</p>
<ol>
<li><p>$Cov(X,Y)=Cov(Y,X)$</p>
</li>
<li><p>$Cov(X+a,Y+b)=Cov(X,Y)$</p>
</li>
<li><p>$Cov(aX,bY)=abCov(X,Y)$</p>
</li>
<li><p>$Cov(X_1+X_2, Y)=Cov(X_1,Y)+Cov(X_2,Y)$</p>
</li>
<li><p>$|Cov(X,Y)| \le \sqrt{D(X)} \sqrt{D(Y)}$</p>
</li>
</ol>
<h3 id="相关系数的性质"><a href="#相关系数的性质" class="headerlink" title="相关系数的性质"></a>相关系数的性质</h3><ol>
<li><p>$|\rho _{_{XY}}| \le 1$</p>
</li>
<li><p>若$X,Y$相互独立,且$D(X),D(Y)$存在,则</p>
</li>
</ol>
<p>$$Cov(X,Y)=\rho _{_{XY}}=0$$</p>
<p>如果随机变量$X$与$Y$的相关系数$\rho _{_{XY}}$,则$X$与$Y$不相关.</p>
<h2 id="矩及协方差矩阵"><a href="#矩及协方差矩阵" class="headerlink" title="矩及协方差矩阵"></a>矩及协方差矩阵</h2><h3 id="概念-4"><a href="#概念-4" class="headerlink" title="概念"></a>概念</h3><p>设$X$和$Y$是随机变量, $k,l$为任一正整数:</p>
<p>(1) 若$E(X^k)$存在,则称$\mu_k=E(X^k)$为$X$的$k$阶原点矩,简称$k$阶距.</p>
<p>(2) 若$E[X-E(X)]^k$存在,则称$\sigma_k=E[X-E(X)]^k$为$X$的$k$阶中心矩.</p>
<p>(3) 若$E(X^kY^l)$存在,则称$\mu_{kl}=E(X^kY^l)$为$X$和$Y$的$k+l$阶混合原点矩.</p>
<p>(4) 若$E{[X-E(X)]^k[Y-E(Y)]^l}$存在,则称$\sigma_{kl}=E{[X-E(X)]^k[Y-E(Y)]^l}$为$X$和$Y$的$k+l$阶混合中心矩.</p>
<p>显然,$X$的一阶原点矩$E(X)$就是$X$的数学期望,$X$的一阶中心矩$E[X-E(X)]$为$X$的偏差的数学期望,其恒等于0,即</p>
<p>$$E[X-E(X)] = E(X)-E(X)=0$$</p>
<p>而$X$的二阶中心矩$E{[X-E(X)]^2}$就是$X$的方差.$X$和$Y$的二阶混合中心矩就是$X$和$Y$的协方差$Cov(X,Y)$.</p>
<h3 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a>协方差矩阵</h3><p>设$n$维随机变量$(X_1,X_2,…,X_n)$的二阶混合中心矩</p>
<p>$$C_{ij}=Cov(X_i,Y_j)=E{[X_i-E(X_i)][Y_j-E(Y_j)]} \qquad i,j=1,2,…n$$</p>
<p>都存在,则称矩阵</p>
<p>$$C=\left \lgroup \begin{matrix} C_{11} &amp; C_{12} &amp; \cdots &amp; C_{1n} \ C_{21} &amp; C_{22} &amp; \cdots &amp; C_{2n} \ \vdots &amp; \vdots &amp; \cdots &amp; \vdots \ C_{n1} &amp; C_{n2} &amp; \cdots &amp; C_{nn} \end{matrix} \right \rgroup$$</p>
<p>为$n$维随机变量$(X_1,X_2,…,X_n)$的协方差矩阵.由于$C_{ij}=C_{ji}(i,j=1,2,…n)$,因而矩阵$C$为一对称矩阵.</p>
<h1 id="大数定律及中心极限定理"><a href="#大数定律及中心极限定理" class="headerlink" title="大数定律及中心极限定理"></a>大数定律及中心极限定理</h1><h2 id="大数定律-LLN"><a href="#大数定律-LLN" class="headerlink" title="大数定律$(LLN)$"></a>大数定律$(LLN)$</h2><p>设$X_1,X_2,…,X_n,…,$是随便变量序列,$E(X_k)(k=1,2,…)$存在. 令$\bar{X}<em>n=\frac{1}{n}\sum</em>{k=1}^{n}X_k$,若对于任意给定正数$\varepsilon &gt; 0$, 有</p>
<p>$$\lim_{n \rightarrow \infty}P{|\bar{X}_n-E(\bar{X}_n)| \ge \varepsilon } = 0$$</p>
<p>或</p>
<p>$$\lim_{n \rightarrow \infty}P{|\bar{X}_n-E(\bar{X}_n)| &lt; \varepsilon } =1 $$</p>
<p>则称${X_n}$服从大数定律或称大数法成立.</p>
<p>(贝努利定理)设$n_A$是$n$次独立重复试验中事件$A$发生的次数,$p$是事件$A$在每次试验中发送的概率,则对于任意的正数$\varepsilon &gt; 0$, 有</p>
<p>$$\lim_{n \rightarrow \infty} P{|\frac{n_A}{n}-p| &lt; \varepsilon } = 1 $$</p>
<p>或</p>
<p>$$\lim_{n \rightarrow \infty} P{|\frac{n_A}{n}-p| \ge \varepsilon } = 0 $$</p>
<p>契比雪夫特殊情况: 设$X_1,X_2,\cdots,X_n,\cdots$相互独立(即对于任意的$n\ge 1, X_1,X_2,\cdots,X_n$是相互独立的), 且具有相同的数学期望和方差$E(X_k)=\mu, D(X_k)=\sigma ^2(k=1,2,\cdots)$, 则对任意正数$\varepsilon &gt; 0$, 有</p>
<p>$$\lim_{n \rightarrow \infty} P {|\bar{X}_n - \mu | &lt; \varepsilon } = 1$$</p>
<p>辛钦定理: 设随机变量序列$X_1,X_2,\cdots,X_n,\cdots$相互独立,服从同一分布,且具有数学期望$E(X_k)=\mu(k=1,2,…)$,则对于任意正数$\varepsilon$有</p>
<p>$$\lim_{n \rightarrow \infty} P{|\bar{X}_n - \mu| &lt; \varepsilon } = 1$$</p>
<p>即</p>
<p>$$\bar{X}_n \stackrel{P} \longrightarrow \mu $$</p>
<h2 id="中心极限定理-CLT"><a href="#中心极限定理-CLT" class="headerlink" title="中心极限定理$(CLT)$"></a>中心极限定理$(CLT)$</h2><p>凡是在一定条件下,断定随机变量序列$X_1,X_2,\cdots$的部分和$Y_n=\sum_{k=1}^{n}X_k$的极限分布为正态分布的定理,均称为中心极限定理.</p>
<p>即中心极限定理应当说明,在何种条件下,下式成立:对任意的$x$,有</p>
<p>$$\lim_{n \rightarrow \infty} P{\frac{Y_n - E(Y_n)}{\sqrt{D(Y_n)}} \le x } = \int_{-\infty}^x \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}dt=\Phi(x)$$</p>
<p>隶莫佛-拉普拉斯定理</p>
<p>设随机变量列$Y_n(n=1,2,\cdots)$服从参数为$n,p$的二项分布$B(n,p)(0&lt;p&lt;1)$,则对于任意的$x$,恒有</p>
<p>$$\lim_{n \rightarrow \infty} P{\frac{Y_n - np}{\sqrt{np(1-p)}} \le x } = \Phi(x)$$</p>
<p>独立同分布中极限定理</p>
<p>设$X_1,X_2,\cdots,X_n,\cdots$相互独立,且服从同一分布,具有数学期望及方差:$E(X_k)=\mu, D(X_k)=\sigma^2 \neq 0(k=1,2,\cdots)$,则随机变量$Y_n=\sum_{k=1}{n}X_k$近似服从正态分布$N(n\mu,n\sigma^2)$,即对于任意的$x$,有</p>
<p>$$\lim_{n\rightarrow \infty}P{\frac{Y_n - n\mu}{\sqrt{n}\sigma} \le x } = \Phi(x)$$</p>
<p>李雅普诺夫定理</p>
<p>设随机变量$X_1,X_2,\cdots,X_n$相互独立,它们具有数学期望和方差:</p>
<p>$$E(X_k)=\mu_k, D(X_k)=\sigma_k^2 \neq 0 \qquad k=1,2,\cdots$$</p>
<p>记$B_n^2=\sum_{k=1}^n\sigma_k^2$,若存在正数$\delta$,使得当$n \rightarrow \infty$时,</p>
<p>$$\frac{1}{B_n^{2+\delta}} \sum_{k=1}^nE{|X_k-\mu_k|^{2+\delta} } \rightarrow 0$$</p>
<p>则随机变量$Y_n=\sum_{k=1}^nX_k$近似服从正态分布$N(\sum_{k=1}^n\mu_k,B_n^2)$, 即对于任意的$x$,有</p>
<p>$$\lim_{n \rightarrow \infty} P{\frac{Y_n-\sum_{k=1}^n \mu_k}{B_n} \le x } = \Phi(x)$$</p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://mejhwu.github.io/2017/10/22/algorithm_binary_search_tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="jhwu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/22/algorithm_binary_search_tree/" itemprop="url">
                  二叉搜索树
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-10-22 00:00:00" itemprop="dateCreated datePublished" datetime="2017-10-22T00:00:00+08:00">2017-10-22</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-06-27 20:17:34" itemprop="dateModified" datetime="2018-06-27T20:17:34+08:00">2018-06-27</time>
              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/算法/" itemprop="url" rel="index"><span itemprop="name">算法</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>二叉搜索树是一种被用于查找的二叉树。其要求是：每个节点都大于其左子树，小于或等于右子树。</p>
<p><img src="../images/binary_search_tree/binary_search_tree.png" alt=""></p>
<h3 id="1-查找"><a href="#1-查找" class="headerlink" title="1. 查找"></a>1. 查找</h3><p>从根节点开始查找。如果关键值等于当前节点值，说明查找成功。如果关键值小于当前节点值，则在当前节点的左子树上查找。如果关键值大于当前节点，则在当前节点的右子树上查找。</p>
<p><img src="../images/binary_search_tree/binary_search_tree_search.png" alt=""></p>
<h3 id="2-插入"><a href="#2-插入" class="headerlink" title="2. 插入"></a>2. 插入</h3><p>插入的节点一定是叶子节点。二叉搜索树插入前需要先进行查找，最后查找到的叶子节点即为插入节点的位置。</p>
<p><img src="../images/binary_search_tree/binary_search_tree_insert.png" alt=""></p>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">jhwu</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">5</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">3</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">jhwu</span>

  

  
</div>




  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> v3.7.1</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Muse</a> v6.3.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script>



  



	





  





  










  





  

  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
  

  


  
  

  

  

  

  

  

</body>
</html>
